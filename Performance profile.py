# -*- coding: utf-8 -*-
"""Performance profile.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WvfGbVFGoja7UIf7iufUhDe5rkoEWLHq
"""

#==============================================================================
# Código de perfis de desempenho e construção do hipervolume graficamente.
# Neste script, utilizamos o google drive como ponte para os arquivos .npz que contém
# os pontos não dominados e o tempo de execução de cada experimento para calcular as métricas
# Hipervolume, Purity, Gamma-Spread e Tempo de Execução do multi-start seguindo as informações do
# trabalho principal, presentes no capítulo 4.
#==============================================================================

#==== GOOGLE DRIVE ==========================================================
#from google.colab import drive
#drive.mount('/content/drive', force_remount=False)

#======== DEPENDÊNCIAS  (instalar no *terminal* do Colab) =======================
!pip install numpy scipy matplotlib pandas cplex docplex
!pip install -q pymoo
# substituimos o pygmo por pymoo: import pygmo as pg

import sys
print("Python executable:", sys.executable)
print("Python version   :", sys.version)

# Instala pymoo no mesmo interpretador do kernel
!{sys.executable} -m pip install -q pymoo


#==== IMPORTS e CONFIG. GERAIS =============================================

import os                    # funções de sistema operacional (paths, variáveis de ambiente)
import glob                 # busca de arquivos ('*.npz')
import warnings             # controle e filtragem de mensagens de warning.
import numpy as np          # vetores/matrizes e cálculo numérico eficiente.
import matplotlib.pyplot as plt  # criação de gráficos 2-D com interface MATLAB.
from pymoo.indicators.hv import HV  # ferramentas de otimização (usado aqui para hipervolume)
import requests             # requisições HTTP simples (download de dados .txt)
import pandas as pd         # estrutura DataFrame para formatação de tabelas mais rápido.


warnings.filterwarnings(
    "ignore", category=UserWarning, module='matplotlib.font_manager')

plt.rcParams.update({
    'text.usetex'     : False,
    'font.family'     : 'serif',
    'font.serif'      : ['Computer Modern'],
    'mathtext.fontset': 'cm'
})

import matplotlib as mpl
mpl.rcParams['font.family'] = 'sans-serif'
mpl.rcParams['font.sans-serif'] = ['Arial', 'Liberation Sans', 'DejaVu Sans']
# ===============================================================
# Dados para problema Caso Simples (n = 6)
# Basta descomentar para utilizar.
# Tudo abaixo de "url.... até mu = v.copy()" deve ser comentado.
# ===============================================================
#n = 6
#v = np.array([0.021, 0.04, -0.034, -0.028, -0.005, 0.006])

#Q = np.array([
#    [0.038, 0.020, 0.017, 0.014, 0.019, 0.017],
#    [0.020, 0.043, 0.015, 0.013, 0.021, 0.014],
#    [0.017, 0.015, 0.034, 0.011, 0.014, 0.014],
#    [0.014, 0.013, 0.011, 0.044, 0.014, 0.011],
#    [0.019, 0.021, 0.014, 0.014, 0.040, 0.014],
#    [0.017, 0.014, 0.014, 0.011, 0.014, 0.046]
#])
#mu = v.copy()   # Para usar em MIQP
# ===============================================================
# Leitura dos dados do problema Port 2 de OR-LIB
# ===============================================================
# O site indicado contém o arquivo port2.txt com a seguinte estrutura:
#      linha 0  – número de ativos (n)
#      linhas 1..n  – retorno médio e desvio padrão de cada ativo
#      Outras linhas – pares (i,j) mais correlação(i,j)
# ===============================================================
#url = 'https://people.brunel.ac.uk/~mastjjb/jeb/orlib/files/port1.txt'
url = 'https://people.brunel.ac.uk/~mastjjb/jeb/orlib/files/port4.txt'
response = requests.get(url)        # Realiza requisição para o endereço url
response.raise_for_status()         # Manda mensagem se falhar
linhas = response.text.splitlines() # Converte em lista de strings (ou seja, linhas)
n = int(linhas[0].strip())          # dimensão do problema de portfólio
retorno_esperado = np.zeros(n)
stds = np.zeros(n)

# Vetor de retornos esperados (v) e desvios padrão (stds)
for i in range(n):
    retorno_medio, std_dev = map(float, linhas[1 + i].split())
    retorno_esperado[i] = retorno_medio
    stds[i] = std_dev

# Montagem da matriz de covariância)
#Passo‑a‑passo:  Q_ii = sigma_i²   e   Q_ij = corr_ij * sigma_i * sigma_j
Q = np.zeros((n, n))
for i in range(n):
    Q[i, i] = stds[i]**2  # Para a diagonal da matriz
for linhas in linhas[1 + n:]:   # Fora da diagonal
    if not linhas.strip(): continue
    i_str, j_str, corr_str = linhas.split()
    i, j = int(i_str)-1, int(j_str)-1 # Usado para pegar uma linha do arquivo que descreve a correlação entre dois ativos
    corr = float(corr_str)            # Conversão do texto do arquivo para ponto flutuante
    cov_ij = corr * stds[i] * stds[j]
    Q[i, j] = Q[j, i] = cov_ij        # Reproduz o simétrico da matriz
v = retorno_esperado
mu = v.copy()   # Para usar em MIQP
#========================= Ajustes aqui ========================================
BASE   = '/content/drive/MyDrive/meus_experimentoscom50starts2'   # pasta com .npz para Port 2, 3 e 4
#BASE   = '/content/drive/MyDrive/meus_experimentos2'    # Caso simples e Port 1.
PENS   = ['quad', 'exp']                               # métodos
COLORS = {'quad': 'blue', 'exp': 'red'}
NS     = [85,89,98] # tamanhos (n) - problemas de portfólio Port 2, 3 e 4.
#NS = [6,31]        # Tamanho (n) - Problemas de portfólio Caso simples e Port 1.
BIG = 5          # penalidade comum para “problema não resolvido”.
# ===============================================================================

#======= CARREGA TODOS OS ARQUIVOS .npz =======================================
conv = {}
for fp in glob.glob(os.path.join(BASE, 'convergence_*_n*.npz')):
    key = os.path.basename(fp).replace('.npz', '')
    conv[key] = np.load(fp, allow_pickle=True)


# =============================================================================
# UTILITÁRIOS
# =============================================================================

def global_referencia(n: int):
    """Retorna ponto‑referência (vetor 2‑D) comum a todos os métodos/α."""
    f1_max, f2_max = -np.inf, -np.inf
    for pen in PENS:
        nd_dict = conv[f'convergence_{pen}_n{n}']['nd_points'].item()
        for pts in nd_dict.values():
            if pts.size == 0:
                continue
            f1_max = max(f1_max, pts[:, 0].max())
            f2_max = max(f2_max, pts[:, 1].max())
    ref = np.array([f1_max, f2_max])+ 1e-3  # 1e-3 é uma folga para o número.
    return ref

def salvo(v, big=BIG):
    """Devolve v se for finito; caso contrário devolve BIG."""
    return v if np.isfinite(v) else big

def hv_from_nd(nd_pts, ref):
    """
    Calcula o hipervolume (valor de Lebesgue) da região dominada pela fronteira "nd_pts"
    e limitada pelo ponto de referência "ref", usando pymoo.indicators.hv.HV.

    Parâmetros
    nd_pts = Matriz com k pontos não-dominados (cada linha é um vetor de m objetivos).
             Se estiver vazia (k == 0) a função devolve a penalidade BIG.
    ref = Ponto de referência que limita a região dominada.
    Retorna
    O hipervolume da região dominada por "nd_pts" até "ref".
    """
    if nd_pts.size == 0:
        return BIG
    hv = HV(ref_point=ref) #O objeto retornado (hv) vai conter o algoritmo/rotina que calcula o hipervolume quando chamado.
    # HV espera uma matriz (k x m).
    return float(hv(nd_pts)) #Retorna no formato float64 do numpy quando chamado, por isso usamos float para converter para o tipo float do Python.


# ===========================================================================
#  CONVERGÊNCIA HV × κ  (Não foi usado no trabalho, assim como Delta-Spread)
# ===========================================================================
#for n in NS:
#    αs_raw = conv[f'convergence_quad_n{n}']['alpha_values']
##    αs     = [int(a) for a in αs_raw]
#    for α in αs:
#        plt.figure(figsize=(5, 2.5))
#        for pen, sty in zip(PENS, ['--', ':']):
#            hv_series = conv[f'convergence_{pen}_n{n}']['hv_conv'].item()[α]
#            plt.plot(hv_series, linestyle=sty, color=COLORS[pen],
#                     marker='o', ms=3, label=f'AL-{pen}')
#        plt.title(fr"$n={n},\ \alpha={α}$ — Hipervolume vs $\kappa$", fontsize=14)
#        plt.xlabel(r'$\kappa$');  plt.ylabel('HV');  plt.grid(True)
#        plt.legend();  plt.tight_layout();  plt.show()


# =============================================================================
# PERFIS DE DESEMPENHO (Dolan-Moré) — HV e Gamma-Spread
# =============================================================================

#  Gamma-Spread com delta_{i,j}=f_{j}^{(i+1)}−f_{j}^{(i)} -------------------
def gamma_spread(front: np.ndarray):
    """fronteira (k x 2) em (0.5x^TQx, -v^Tx).  Gamma = max_j max_i delta_{i,j}."""
    if front.shape[0] < 2:            # frente com 0 ou 1 ponto => nenhuma lacuna
        return 0.0

    # Adiciona os pontos extremos (x0 e xN+1)
    x0 = np.min(front, axis=0)  # ponto extremo com o melhor valor
    xN1 = np.max(front, axis=0)  # ponto extremo com o pior valor
    fronteira_com_extremos = np.vstack([x0, front, xN1])

    # Inicializa a métrica Gamma-Spread
    g = 0.0
    for j in range(fronteira_com_extremos.shape[1]):   # j = 0 (f1) ou 1 (f2)
        v = np.sort(fronteira_com_extremos[:, j])      # ordena valores de f_j
        d = v[1:] - v[:-1]            # delta = [delta_{0,j}, delta_{1,j}, …]
        g = max(g, d.max())           # Gamma = max_j (max_i delta_{i,j})

    return float(g)

#==================== loop principal de tamanhos n =============================
metrics = [
    # key            bigger invert logx   rótulo do gráfico
    ('hv_global'   , False , True , False, 'Hipervolume'),
 #   ('spread_conv' , False , False, False, 'Δ-Spread'),
    ('gamma_global', False , False, False , 'Γ-Spread'),
]
# Em metrics, "key" é a metrica em uso, "bigger" é se essa métrica é do tipo que minimiza (False) ou máximiza (True)
# Não foi usado, porque a função "invert" já inverte a métrica hipervolume (1/HV) e Gamma-Spread já é uma métrica de minimizar
# O "logx" é caso precisassemos logx mudar a escala dos eixos para log_10, caso a quantidade de dados fosse muito grande, mas não foi preciso.

# Simbolo α salvo para usar.
#'alpha_values'= lista dos valores de α testados.
# "nd_points"  dicionário de pontos não dominados vindos do .npz, para cada α, uma matriz k x 2

for n in NS:  #repetimos todo o processo para cada instância do problema n=85,89,98
  #  αs = conjunto de valores de cardinalidade testados naquele problema
    αs = conv[f'convergence_quad_n{n}']['alpha_values'].tolist()

    # Hipervolume final ( usando referência global)========================
    ref = global_referencia(n)
    hv_global = {pen: [] for pen in PENS}
    for pen in PENS: # pen pertence a PENS={'quad','exp'}
        nd_dict = conv[f'convergence_{pen}_n{n}']['nd_points'].item() #Recupera o dicionário de pontos não dominados de AL-Quad ou AL-Exp .npz
        for α in αs:  #Percorre todos os α testados no problema de portfólio
            hv_global[pen].append(hv_from_nd(nd_dict[α], ref)) #Armazena o resultado na lista hv_global[pen], no fim temos hv_gloval={'quad':[HV_a_1,HV_a_2,...,HV_a_25], 'exp':...}

    # 2-B) Gamma-Spread final (por α e por método)=============================
    gamma_global = {pen: [] for pen in PENS}
    for pen in PENS: # pen pertence a PENS={'quad','exp'}
        nd_dict = conv[f'convergence_{pen}_n{n}']['nd_points'].item() #Recupera o dicionário de pontos não dominados de .npz
        for α in αs: #Percorre todos os α testados no problema de portfólio
            gamma_global[pen].append(gamma_spread(nd_dict[α])) #Armazena o escalar Gamma_Spread na cobertura da frente para aqule alpha

    # guarda para uso no loop das métricas
    salva_metricas= {'hv_global': hv_global,'gamma_global': gamma_global}

    # loop das duas métricas
    for key, bigger, inv, logx, label in metrics:        #metrics é de nossa tabela de configuração
        #=========== coleta valores =========================================
        perf = {pen: [] for pen in PENS} #inicia com o dicionário na forma {'quad':[], 'exp': []}
        if key in salva_metricas:             # se key é hv hv_global ou gamma_global, pega os dados de salva_metricas
            perf = salva_metricas[key]
        else:                                 # do contrário spread_conv já gravada ao longo da κ (não usamos na prática)
            #for pen in PENS:
            #    arr = conv[f'convergence_{pen}_n{n}'][key].item() #arr devolve outro dicionário.
            #    perf[pen] = [arr[α][-1] if hasattr(arr[α], '__len__') else arr[α] #arr[α] é a lista de valores ao longo das iterações k
             #                 for α in αs] #se for hasattr(arr[α], '__len__') usamos o último valor ([-1]) que representa o Delta-spread
            continue
 #      Exemplo do que temos perf =  {'quad': [ valor_α_1, valor_α_2,..., valor_α_m ],
 #                                     'exp' : [ valor_α_1, valor_α_2, ..., valor_α_m ]}

        # ============= inversão (HV) ============================
        # Nesta parte a inversão vai ser feita apenas com a métrica de hipervolume, pois "inv" está True, transformamos então 1/HV
        for pen in PENS: # pen pertence a PENS={'quad','exp'}
            perf[pen] = [1/p if inv and p > 0 else p for p in perf[pen]]
            perf[pen] = [salvo(v) for v in perf[pen]]        # Garante que divisões não explodam (NaN/inf => BIG)

        # --------- razões de desempenho r_{p,s} >= 1 ========================
        # m=len(αs) é o número total de experimentos (cada alpha corresponde a uma instância do mesmo problema n, mas com card. diferente).
        # Teremos cada experimento s correspondendo a uma coluna em perf[pen], onde cada coluna representa um alpha no problema de dimensão n.
        ratios = {pen: [] for pen in PENS} #Cria dicionário vazio { 'quad': [], 'exp': [] } para guardar as razões r_{p,s} em cada método p naquele alpha.
        for i in range(len(αs)): #Vai percorrer a coluna i=0,...,m-1, onde cada coluna corresponde ao valor de um alpha específico.
            best = (max if bigger else min)(perf[p][i] for p in PENS) #Seleciona o melhor valor na coluna i (cada alpha) entre os dois métodos:
            #Usa min se Bigger=False (menor é melhor) e max se for True (maior é melhor)
            #perf[p][i] for p in PENS, temos PENS=[Quad, Exp] e perf[p][i] produz para cada coluna i, "perf[quad][i], perf[exp][i]".
            #Na sequência, obtemos o best (valor que define o vencedor para aquele alpha) dependendo de Bigger
            for pen in PENS:
                v = perf[pen][i]  # Valor da métrica para o método "pen" no α atual
                ratios[pen].append(v / best if best != 0 else BIG)  # Usar BIG se o melhor valor for zero

        # ===================== domínio τ  ==================================
        # τ salvo para uso
        # np.hstack vai concatenar as duas listas de razões (quad+exp)
        # np.unique devolve os valores distintos em ordem crescente
        τs = np.unique(np.hstack(list(ratios.values())))

        # ==================== gráfico final ===============================
        plt.figure(figsize=(6, 4))
        for pen in PENS:
           #np.mean([]) faz a média para (1/|S|)sum(r_{p,s}<=1)
           #plt.step vai construir a curva em degrau
           # O título inclui automanticamente a dimensão n e o nome da métrica
           # logx é a metrica log_10 caso o tamanho dos dados fosse muito grande, mas não foi preciso
            ρ = [np.mean([r <= tau for r in ratios[pen]]) for tau in τs] #realiza a avaliação de desempenho dos métodos com base nas razões
            #"for tau in τs" percorre todos os valores distintos de τ no vetor τs que é a lista de diferentes valores de referência
            # "[r <= tau for r in ratios[pen]]" para cada τ, criamos uma lista de booleanos e para cada razão de desempenho r na lista ratios[pen]
            # do método escolhido, verificamos se r<=τ,o resultado é uma lista de False/True
            # np.mean calcula a média dos valores booleanos, tratamos True como 1 e False como 0.
            plt.step(τs, ρ, where='post', linestyle='--',
                     color=COLORS[pen], label=f'AL-{pen.capitalize()}')
        plt.title(fr"$n={n}$ — Perfil: {label}", fontsize=18)
        plt.xlabel(r'$\tau$', fontsize=18)
        plt.ylabel(r'$\rho(\tau)$', fontsize=18)
        if logx:
            plt.xscale('log', base=10)
        plt.grid(True);  plt.legend(fontsize=16);  plt.tight_layout();  plt.show() #Ajustes na imagem como grade, legenda e margens



import matplotlib as mpl

mpl.rcParams['font.family'] = 'sans-serif'
mpl.rcParams['font.sans-serif'] = ['Arial', 'Liberation Sans', 'DejaVu Sans']

# =============================================================================
# TEMPO CPU — Perfil de Desempenho
# =============================================================================
#'alpha_values'= lista dos valores de α testados.
# "time_conv" = dicionário de tempos time_conv[α] exportado por .npz de cada método.
for n in NS:
    αs = conv[f'convergence_quad_n{n}']['alpha_values'].tolist()
    tempos = {pen: conv[f'convergence_{pen}_n{n}']['time_conv'].item() for pen in PENS}

    # ==== calcula o tempo total de t ========
    "Para cada método de penalização e α, fazemos T_{p,α}=sum_i(T^{i}_{p,α}) "
    t_total = {pen: [sum(tempos[pen][α]) if len(tempos[pen][α]) > 0 else BIG
                 for α in αs]
           for pen in PENS}

    # ================= razões de desempenho =============================
    ratios = {pen: [] for pen in PENS}
    for i in range(len(αs)):  #Para percorrer alpha_1, alpha_2,....
        best = min(t_total[p][i] for p in PENS)   # menor TOTAL, quem foi o mais rápido.
        best = max(best, 1e-8)                    # evita divisão por 0.
        for pen in PENS:
            val = salvo(t_total[pen][i])           # BIG se nan ou inf.
            ratios[pen].append(val / best)         # adiciona na lista ratios[pen] r

    # ========= domínio τ (todos os ratios observados) ==================
    # np.hstack vai concatenar as duas listas de razões (quad+exp)
    # np.unique devolve os valor distintos em ordem crescente
    τs = np.unique(np.hstack(list(ratios.values())))

    # ========== gráfico tempo de CPU ==========================================
    plt.figure(figsize=(6, 4))
    for pen in PENS:
        ρ = [np.mean([r <= tau for r in ratios[pen]]) for tau in τs] #realiza a avaliação de desempenho dos métodos com base nas razões
            #"for tau in τs" percorre todos os valores distintos de τ no vetor τs que é a lista de diferentes valores de referência
            # "[r <= tau for r in ratios[pen]]" para cada τ, criamos uma lista de booleanos e para cada razão de desempenho r na lista ratios[pen]
            # do método escolhido, verificamos se r<=τ,o resultado é uma lista de False/True
            # np.mean calcula a média dos valores booleanos, tratamos True como 1 e False como 0.
        plt.step(τs, ρ, where='post', linestyle='--', color=COLORS[pen], label=f'AL-{pen.capitalize()}')
    plt.title(fr"$n={n}$ — Perfil: Tempo de CPU", fontsize=18)
    plt.xlabel(r'$\tau$',fontsize=18)
    plt.ylabel(r'$\rho(\tau)$',fontsize=18)
    plt.grid(True)
    plt.legend(fontsize=16)
    plt.tight_layout()
    plt.show()


# =============================================================================
# Métrica Purity
# =============================================================================
LOG_PURITY = False          # Possibilidade de usar a escala logaritmica de base 10
# ============================== Funções auxiliares =============================
def domina(a, b):
  """
  Função de dominância
  a <= b é um vetor booleano (True ou False) com o resultado dessa comparação componente a componente (a_i <= b_i)
  all() vai devolver True apenas quando todas as comparações forem True, garantindo a_i <= b_i
  a<b cria outro vetor booleano agora para a_i<b_i
  any() vai devolver True se alguma componente do vetor é True
  and para que ambas as condições sejam satisfeitas, como consta a definição presente no Capítulo 2 do trabalho.
  """
  return (a <= b).all() and (a < b).any()

def pareto(points):
    nd = [] # Começamos com lista de pontos não dominados vazia.
    for p in points: # vamos percorrer cada condidato p de poinst.
     #domina(np.array(q), np.array(p)), para o ponto p, testa todos os outros q, verificando se algum deles domina p, e any(...) devolve True se encontrar um q que domina p.
        if not any(domina(np.array(q), np.array(p)) for q in points if q is not p):
          # Se nenhum q domina p, dizemos que p é não dominado.
            nd.append(p) #adiciona p à lista nd.
    return nd    #vai devolver a lista com todos os não dominados

BIG = 5  # penalidade quando pureza = 0

for n in NS:
   #Percorrendo cada dimensão n, relembrando que αs é a lista completada de cardinalidade usadas no problema de portfólio.
    αs = conv[f'convergence_quad_n{n}']['alpha_values'].tolist()
    nd_q = conv[f'convergence_quad_n{n}']['nd_points'].item() #
    nd_e = conv[f'convergence_exp_n{n}']['nd_points'].item()  # São as frentes individuais no ND

    purity_q, purity_e = [], []
    for α in αs:
      #np.round(..., 12) evita ruído numérico que atrapalha a comparação.
        Pq = [tuple(p) for p in np.round(nd_q[α], 12)] #É a frente F_{Quad,s} convertida em tupla e arredondada (12 casas) para comparação.
        Pe = [tuple(p) for p in np.round(nd_e[α], 12)] #É a frente F_{Exp,s} convertida em tupla e arredondada (12 casas) para comparação.
        # ============ Fronteira de Pareto aproximada ======================
        global_front = pareto(list(set(Pq + Pe)))
        # set(Pq + Pe) Une as frentes dos dois métodos e remove também duplicatas exatas antes de usar pareto() que elimina todos os pontos dominados dessa união.
        # global_front é a fronteira não dominada dessa união, ou seja, F_p.
        # ============= contagem ===========
        purity_q.append(len([p for p in Pq if p in global_front]) / len(Pq) if Pq else 0) #len([p for p in Pq if p in global_front]) (numerador) é a quantidade de pontos de AL-Quad que ainda estão em F_p
        purity_e.append(len([p for p in Pe if p in global_front]) / len(Pe) if Pe else 0) #len(Pq) (denominador) é ototal de pontos da fronteira de Pareto de AL-Quad
        #Essa divisão implementa |F_p \cap F_{p,s}|/|F_{p,s}|
    tq = [1/p if p > 0 else BIG for p in purity_q] #invertendo como hipervolume (AL-Quad)
    te = [1/p if p > 0 else BIG for p in purity_e] ##invertendo como hipervolume (AL-Exp)
    best_q, best_e = min(tq), min(te)
    ratios_q = [t / best_q for t in tq] #Para AL-Quad, best_q indicará a melhor medida para purity, que é equivalente a ser a maior purity
    ratios_e = [t / best_e for t in te] # Para cada alpha, obtemos r_{p,s}>=1

    # ========= domínio τ (todos os ratios observados) ==================
    # np.hstack vai concatenar as duas listas de razões (quad+exp)
    # np.unique devolve os valor distintos em ordem crescente
    τs = np.unique(np.hstack([ratios_q, ratios_e]))

     # ========== gráfico Purity ==========================================
    plt.figure(figsize=(6, 4))
    ρq = [np.mean([r <= tau for r in ratios_q]) for tau in τs] #realiza a avaliação de desempenho dos métodos com base nas razões
            #"for tau in τs" percorre todos os valores distintos de τ no vetor τs que é a lista de diferentes valores de referência
            # "[r <= tau for r in ratios_q]" para cada τ, criamos uma lista de booleanos e para cada razão de desempenho r na lista ratios[pen]
            # do método escolhido, verificamos se r<=τ,o resultado é uma lista de False/True
            # np.mean calcula a média dos valores booleanos, tratamos True como 1 e False como 0.
    ρe = [np.mean([r <= tau for r in ratios_e]) for tau in τs]
    plt.step(τs, ρq, where='post', linestyle='--', color=COLORS['quad'], label='AL-Quad')
    plt.step(τs, ρe, where='post', linestyle='--', color=COLORS['exp'] , label='AL-Exp')

    plt.title(fr"$n={n}$ — Perfil: Purity",fontsize=18)
    if LOG_PURITY:                          #  só aplica log se True
        plt.xscale('log', base=10)

    plt.xlabel(r'$\tau$',fontsize=18)
    plt.ylabel(r'$\rho(\tau)$',fontsize=18)
    plt.grid(True);  plt.legend(fontsize=16, loc='lower right');  plt.tight_layout();  plt.show()


import matplotlib as mpl

mpl.rcParams['font.family'] = 'sans-serif'
mpl.rcParams['font.sans-serif'] = ['Arial', 'Liberation Sans', 'DejaVu Sans']


#====== Retângulos de hipervolume AL-quad + AL-exp + MIQP =====================
import matplotlib.pyplot as plt, matplotlib.patches as patches, numpy as np
from docplex.mp.model import Model
import time, pandas as pd

# parâmetros básicos
n_fig  = 98  # n_fig = 6/31
alpha  = 98  # alpha = 6/31
lambdas = np.linspace(0, 1, 100)   # lambdas em MIQP CPLEX
cores_AL = {'quad': 'royalblue',   # retângulos / escada AL-quad
            'exp' : 'red'}      # retângulos / escada AL-exp

# =============================================================================
# Função auxiliar: extrai e converte a frente AL (em risco, retorno)
# =============================================================================
# Vai recuperar pts (frente ND do método AL) para os parâmetros n_fig (n° de ativos)
# e alpha (cardinalidade)
def frente_AL(metodo):
    pts = conv[f'convergence_{metodo}_n{n_fig}']['nd_points'].item().get(alpha,np.empty((0,2))) #.npz com todas as execuções
    if pts.size == 0: # Se a frente está vazia, sem retângulo.
        raise ValueError(f"Nenhuma frente ND para {metodo}, n={n_fig}, α={alpha}")
    pts = pts[np.argsort(pts[:,0])]                       # pts[:,0] – primeiro objetivo, np.argsort(...) vai devolver um vetor de índices que ordena esse array em ordem crescente.
    # indexando pts[...] dessa forma, reordenamos a matriz para que risco_1<risco_2<..., porque a linha em degraus no plot precisa começar no menor risco (eixo x) e caminhar para a direita
    risco   = np.sqrt(2.0 * pts[:,0])                     # risco = sqrt(2 * f1)
    retorno = -pts[:,1]                                   # retorno = -f2
    return np.column_stack([risco, retorno])     # Empilha em matriz

# =============================================================================
# MIQP ponderado usando CPLEX
# Descrição do método e da função no trabalho, Capítulo 4.
# =============================================================================
def solve_weighted(alpha, lam, n_=Q.shape[0]):
    """
    Resolve o MIQP ponderado usando CPLEX/DOcplex para um dado valor de alpha e peso lambda.

    Parâmetros
    alpha = Cardinalidade trabalhada.
    lam = Peso do problema ponderado no intervalo [0, 1]

    Retorna
    Se a resolução for bem-sucedida, retorna um dicionário com os campos:
    alpha = Valor de alpha
    lambda = Valor do peso
    f1 =  0.5*x^TQx (variância)
    f2 = -v^Tx (objetivo de retorno negativo)
    risco = sqrt(x^TQx)
    ret = v^Tx
    card = cardinalidade de x
    viavel = resultado da verificação de viabilidade
    msgs = mensagens de verificação (quando aplicáveis)
    time = tempo de resolução em segundos
    Retorna None caso o solver não encontre solução.
    """
    mdl = Model(name='miqp_weighted')
    x = mdl.continuous_var_list(n_, lb=0, ub=1)
    y = mdl.binary_var_list(n_)
    mdl.add_constraints(x[i]+y[i] <= 1 for i in range(n_))
    mdl.add_constraint(mdl.sum(x) == 1)
    mdl.add_constraint(mdl.sum(y) >= n_ - alpha)
    quad = mdl.sum(Q[i,j]*x[i]*x[j] for i in range(n_) for j in range(n_))/2
    lin  = mdl.sum(-mu[i]*x[i] for i in range(n_))
    mdl.minimize((1-lam)*quad + lam*lin)
    mdl.parameters.timelimit = 120
    t0 = time.time()
    sol = mdl.solve(log_output=False)
    t1 = time.time()
    if sol is None:
        return None
    xs = np.array([sol.get_value(xi) for xi in x])
    return dict(lambda_=lam,
                f1=float(xs @ (Q @ xs) / 2),
                f2=float(-mu @ xs),
                risco=np.sqrt(xs @ (Q @ xs)),
                ret=float(mu @ xs),
                time=t1 - t0)

# =============================================================================
# Resolve MIQP para uma quantidade de lambdas
# =============================================================================
sols = [res for lam in lambdas if (res := solve_weighted(alpha, lam))]
# Estemos aqui construindo a frente MIQP para visualização na figura, o operador " := " está
# chamando a função solve_weighted. Logo, cada item "res" contém {'lambda_', 'f1', 'f2', 'risco', 'ret', 'time'}
nd = []  # Lista vazia para armazenar pontos não dominados

for s in sols:  # Percorre todas as soluções
    F_orig = np.array([s['f1'], s['f2']])  # Objetivos da solução s
    dominado = False  # Inicializa a variável para verificar se s é dominado

    # Primeiro laço: Verifica se a solução s é dominada por algum ponto já em nd
    for zp in nd:
        Fp = np.array([zp['f1'], zp['f2']])  # Objetivos do ponto zp em nd
        if np.all(Fp <= F_orig) and np.any(Fp < F_orig):  # zp domina s
            dominado = True  # Se s for dominada por zp, marca como dominada
            break  # Não há necessidade de continuar a verificação, pois já sabemos que s é dominada

    # Se s for dominada, passamos para a próxima solução
    if dominado:
        continue

    # Segundo laço: Remove pontos dominados de nd por s
    new_nd = []  # Nova lista de não dominados sem os pontos dominados por s
    for zp in nd:
        Fp = np.array([zp['f1'], zp['f2']])  # Objetivos do ponto zp em nd
        # Se zp não é dominado por s, mantém em nd
        if not (np.all(F_orig <= Fp) and np.any(F_orig < Fp)):
            new_nd.append(zp)

    # Adiciona a solução s, pois ela não é dominada por nenhum ponto em nd
    new_nd.append(s)

    # Atualiza a lista nd com a nova lista de não dominados
    nd = new_nd

# Agora, nd contém todos os pontos não dominados após todas as iterações


df_nd = pd.DataFrame(nd) #Organiza a lista ND em dataframe para ser melhor na hora de imprimir a tabela
pts_miqp = df_nd[['risco', 'ret']].values # extraímos apenas as colunas risco e ret para plot
if pts_miqp.size == 0: #exceção se a frente for vazia.
    raise RuntimeError(f"MIQP sem ND para α={alpha}")

# =============================================================================
# Ponto-referência global (risco, retorno)
# =============================================================================
ref_obj = global_referencia(n_fig)                           # (1/2var , −ret)
ref_pt  = np.array([np.sqrt(2 * ref_obj[0]), -ref_obj[1]])  # (risco, retorno)
print(f" ponto-referência global (risco, retorno) = {ref_pt}")

# =============================================================================
# Gera uma figura para cada método AL (quad e exp) + MIQP
# =============================================================================
for metodo in ['quad', 'exp']:
    cor = cores_AL[metodo]
    frente = frente_AL(metodo)

    fig, ax = plt.subplots(figsize=(7, 5))

    # =================== Retângulos de hipervolume =====================
    prev_ret = ref_pt[1]         # começamos no retorno do pto-referência
    for x, r in frente:           # A frente já está ordenada por risco (eixo x)
        ax.add_patch(patches.Rectangle(
                       (x, prev_ret), # canto inferior-esquerdo
                       ref_pt[0] - x, # largura do triângulo = x_ref – x
                       r - prev_ret,  # altura = retorno autal - retorno anterior
                       facecolor=cor,
                      edgecolor=cor, alpha=0.20))
        prev_ret = r    # próximo retângulo vai empilhar emcima

    # ======================== Escada da frente AL ========================
    sx = [frente[0, 0]]
    sy = [ref_pt[1]]
    for x, r in frente:
        sx += [x, x]
        sy += [sy[-1], r]
    sx.append(ref_pt[0])
    sy.append(frente[-1, 1])

    ax.step(sx, sy, where='post', color=cor, lw=2,
            label=f'AL-{metodo.capitalize()} α$=${alpha}')
    ax.scatter(frente[:, 0], frente[:, 1], color=cor, s=35, zorder=3)

    # ========================= Pontos MIQP =========================
    ax.scatter(pts_miqp[:, 0], pts_miqp[:, 1],
               marker='s', facecolors='none', edgecolors='black',
               s=120, lw=2, zorder=6, label='MIQP α$=${}'.format(alpha))

    # ====================== Ponto-referência =======================
    ax.scatter(*ref_pt,
               marker='o', s=90,
               color='black', edgecolor='black', lw=1.5,
               zorder=21, clip_on=False,
               label='Ponto de referência')

    # =================== Limites dos eixos ==========================
    xmin = min(ref_pt[0], pts_miqp[:, 0].min(), frente[:, 0].min())
    xmax = max(ref_pt[0], pts_miqp[:, 0].max(), frente[:, 0].max())
    ymin = min(ref_pt[1], pts_miqp[:, 1].min(), frente[:, 1].min())
    ymax = max(ref_pt[1], pts_miqp[:, 1].max(), frente[:, 1].max())

    ax.set_xlim(xmin * 0.97, xmax * 1.03)
    ax.set_ylim(ymin * 0.97, ymax * 1.03)

    # ================= Estética da figura ==========================
    #ax.set_title(rf"$n=${n_fig}, $\alpha$=${alpha}$ — Construção do Hipervolume",
    #             fontsize=15)
    ax.set_xlabel(r"Risco ", fontsize=18)
    ax.set_ylabel(r"Retorno ",fontsize=18)
    ax.tick_params(axis='x', labelsize=16)
    ax.tick_params(axis='y', labelsize=16)
    ax.grid(True)
    ax.legend(fontsize=12)
    plt.legend()
    plt.tight_layout()
    plt.show()

# =============================================================================
# Tabela-resumo MIQP
# =============================================================================
print(f"\n--- MIQP não-dominadas (α = {alpha}) ---")
print(df_nd[['lambda_', 'f1', 'f2', 'risco', 'ret', 'time']] #dataframe com apenas essas colunas na ordem que queremos (uso do pandas)
      .to_string(index=False, #índice de Data frame desligados
                 float_format="%.6f")) # to_string percorre as colunas selecionadas, aplica essas formatações e devolve uma tabela com essas colunas.

# ========================================================
# Cobertura do hibervolume para o α escolhido
# ========================================================
# df_nd contém os pontos MIQP não-dominados
pts_miqp = df_nd[['f1', 'f2']].values       # (0.5 variância , −retorno)


if pts_miqp.size == 0:
    raise RuntimeError(f"MIQP não possui frente ND para α={alpha}")

# ponto-referência global
ref = global_referencia(n_fig)             # (0.5var_max , −ret_min)

hv = HV(ref_point=ref)
hv_miqp = float(hv(pts_miqp))


for pen in ['quad', 'exp']:
    pts_al = conv[f'convergence_{pen}_n{n_fig}']['nd_points'].item()[alpha]
    if pts_al.size == 0:
        print(f"AL-{pen}: nenhuma frente ND para α={alpha}")
        continue

    hv_al = float(hv(pts_al))
    cobertura  = 100 * hv_al / hv_miqp

    print(f"AL-{pen:<4s} cobre {cobertura:5.1f}% do hipervolume MIQP "
          f"(HV_AL={hv_al:.4e}, HV_MIQP={hv_miqp:.4e})")